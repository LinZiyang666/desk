下面给你一个**单一有效方案**：
**“绕过 `processor.__call__` 对音频强制 `padding="max_length"` 的打包；在 `collate_fn` 里用 `feature_extractor(..., padding=False)` 重新提取音频特征，并把你原来‘对齐到最长’的逻辑改成‘只按真实最短截断’；同时把音频编码器的 `max_source_positions` 提高”。**
这样 5 秒和 2 分钟音频进入 AudioStage 的“有效帧数”将显著不同，耗时就会随长度变化。

---

# 修改清单（逐条写清“把什么改成什么”）

> 说明：下面用到的变量名 `processor`/`proc` 按你脚本里实际名字替换；`noise_path` 指你当前喂入的音频路径变量（如果你是每条样本各自有音频路径，就用对应变量）。

---

## 1）在文件头部：导入 torchaudio

**原来：**

```python
# 没有导入 torchaudio
```

**修改为：**

```python
import torchaudio
import torchaudio.functional as AF
```

---

## 2）提升音频编码器长度上限（加载模型后立即设置）

**原来：**

```python
model = ...  # 你现有的加载
# 没有设置音频编码器的 max length
```

**修改为：**

```python
model = ...  # 你现有的加载
# 将卷积后的时序上限从默认(例如1500)提高，避免长音频在编码器里再次被“卡顶”
if hasattr(model, "audio_tower") and hasattr(model.audio_tower, "config"):
    try:
        model.audio_tower.config.max_source_positions = 6000  # 可按需再调大/调小
    except Exception as e:
        print("[warn] cannot set max_source_positions:", e)
```

> 注：如果随后报“位置编码超界”之类错误，把 6000 适当下调（比如 4000）；如果一切正常，也可以再上调观察更强的线性时长差异。

---

## 3）在 `collate_fn`：对音频“重新提特征 + 只按最短截断”

### 3.1 先保留你现有的 `apply_chat_template` 调用，但**不要信任它的音频字段**

**原来：**

```python
pack = processor.apply_chat_template(
    messages,
    return_tensors="pt",
    return_dict=True,
    padding="max_length",
    max_length=512,
    truncation=True
)
# 直接使用 pack["input_features"], pack["feature_attention_mask"] 作为音频输入
```

**修改为：**

```python
pack = processor.apply_chat_template(
    messages,
    return_tensors="pt",
    return_dict=True,
    padding="max_length",
    max_length=512,
    truncation=True
)
# 立刻用 feature_extractor 重新提音频，覆盖 pack 里的音频字段（绕过 __call__ 的 max_length）
waveform, sr = torchaudio.load(noise_path)  # 或你每条样本的 audio_path
if sr != 16000:
    waveform = AF.resample(waveform, sr, 16000)
# Whisper 风格：单通道，float32，padding=False 以保留真实长度
wave = waveform.mean(dim=0, keepdim=True) if waveform.size(0) > 1 else waveform  # 转单声道
audio_out = processor.feature_extractor(
    wave.squeeze(0).numpy(),  # (T,)
    sampling_rate=16000,
    padding=False,
    return_attention_mask=True
)

# 覆盖 pack 中的音频键
pack["input_features"] = torch.tensor(audio_out["input_features"], dtype=torch.float32)  # [1, n_mels, T_feat]
pack["feature_attention_mask"] = torch.tensor(audio_out["attention_mask"], dtype=torch.int32)  # [1, T_feat]
```

> 这样 pack 里的音频帧长 `T_feat` 就是**真实帧数**（短音频短、长音频长），不再被 `max_length` 固定。

### 3.2 把你原来“对齐到最长”的逻辑，改成“只按最短截断”

**原来（示例）：**

```python
feats = pack["input_features"]             # [B, n_mels, T_feat]
mask  = pack["feature_attention_mask"]     # [B, T_mask]
T_feat = feats.shape[-1]
T_mask = mask.shape[-1]
T_target = max(T_feat, T_mask)             # ← 对齐到最长（放大差异/补零）
# 下面把 features/mask pad 到 T_target ...
```

**修改为：**

```python
feats = pack["input_features"]             # [B, n_mels, T_feat]
mask  = pack["feature_attention_mask"]     # [B, T_mask]
T_feat = feats.shape[-1]
T_mask = mask.shape[-1]
T_target = min(T_feat, T_mask)             # ← 只按最短截断，保留真实有效帧
# 截断到共同最短长度；不做右侧补零扩张
pack["input_features"] = feats[..., :T_target]
pack["feature_attention_mask"] = mask[..., :T_target]
```

> 这一步确保 AudioStage 看到的 `feature_attention_mask.sum()` 随音频时长**真实变化**，不会被你自己的“对齐到最长”再次抹平。

---

## 4）可选但强烈建议：加入最小调试打印（一次即可）

**原来：**

```python
# 没有显式打印有效帧
```

**修改为：**

```python
if step < 3 and rank == 0:  # 只在前几个 step 打印看一眼
    eff = pack["feature_attention_mask"].sum(dim=1).tolist()
    print(f"[collate_fn] audio feats {tuple(pack['input_features'].shape)}, "
          f"mask {tuple(pack['feature_attention_mask'].shape)}, "
          f"effective_frames={eff}")
```

---

# 使用与预期

- 应用以上改动后：

  - 5 秒音频：`effective_frames` 明显小；
  - 2 分钟音频：`effective_frames` 明显大；
  - 进入 AudioStage 的 `feature_lens` / `aftercnn_lens` 会随之增大；
  - 在 CPU/GPU 上你都应看到**耗时随长度上升**（GPU 更明显）。

- 如果两段音频仍然“差不多快”：
  1）打印 `aftercnn_lens.tolist()`（AudioStage 内）；
  2）如果长音频被**卡在同一个上限**，说明编码器侧上限仍不足，把 `max_source_positions` 再调大（如 8000）；
  3）若出现“位置编码长度不够”的报错，再按报错提示把该值略降。

---

需要我把这些修改直接合成一版你的 `qwen2.5_omni_3b_devide_head.py` 完整片段给你贴过来吗？我可以把 `collate_fn` 的相关部分和“加载模型后的设置”打包成可直接替换的版本。
